<!DOCTYPE html>
<html>

<head>
    <title>MANDLOH</title>
</head>
<style>
    body {
        background-color: rgb(150, 150, 150);
        -webkit-user-select: none;
        color: white;
        font-size: 20px;
    }

    button {
        width: 80px;
        height: 50px;
        margin: 2px 1px 2px 1px;
        font-size: 13px;
        color: white;
        background-color: rgb(51, 51, 51);
        border: none;
        border-radius: 5px;
        box-shadow: 0 9px rgba(104, 104, 104, 0.5);
    }

    button:active {
        box-shadow: 0 5px rgba(104, 104, 104, 0.5);
        transform: translateY(4px);
    }
</style>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"> </script>

<body>
    <div style="position:relative">
        <video id="video"></video>
        <canvas id="canvas"></canvas>
        <button onclick="predict()">Predict</button>
    </div>
</body>

<script>
    //접속환경 구분(PC/MOBILE)
    const filter = "win16|win32|win64|mac|macintel";
    const platform = (filter.indexOf(navigator.platform.toLowerCase()) > 0) ? "PC" : "MOBILE";
    console.log(`Client platform : ${platform}`);

    //GUI 설정
    const w = 300, h = 300;
    const video = document.getElementById("video");
    [video.width, video.height] = [w, h];

    //WebRTC
    navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
    let constraints;
    let stream = null;
    let camera = "environment" // "environment" or "user"
    const successCallback = (_stream) => {
        stream = _stream;
        video.srcObject = _stream;
        video.play();
    }

    const change_camera = () => {
        if (platform == "PC") {
            console.log("Not supported on PC.");
        } else {
            if (stream == null) return;
            stream.getTracks().forEach(t => {
                t.stop();
            });
            if (camera == "environment") camera = "user";
            else camera = "environment";
            constraints = { video: { facingMode: { exact: camera }, width: { exact: w }, height: { exact: h } }, audio: false };
            navigator.getUserMedia(constraints, successCallback, (e) => console.log(e));
        }
    }

    if (platform == "PC") {
        constraints = { video: { width: { exact: w }, height: { exact: h } }, audio: false };
    } else {
        constraints = { video: { facingMode: { exact: camera }, width: { exact: w }, height: { exact: h } }, audio: false };
    }
    navigator.getUserMedia(constraints, successCallback, (e) => console.log(e));

    const canvas = document.getElementById("canvas");
    [canvas.width, canvas.height] = [w, h];
    const ctx = canvas.getContext("2d");
    ctx.fillStyle = "rgb(0, 255, 0)";
    ctx.strokeStyle = "rgb(0, 255, 0)";
    ctx.font = "13px Arial";
    ctx.lineWidth = 3;

    let model;
    const load = async () => {
        const url = "https://raw.githubusercontent.com/grasskin/fastdepth_tfjs/master/fastdepth_opset9_v2_tfjs/model.json";
        model = await tf.loadGraphModel(url);
        console.log("Model loaded");
    }
    load();

    const predict = async () => {
        const image = tf.browser.fromPixels(video).resizeBilinear([224, 224]).transpose([2, 0, 1]).reshape([1, 3, 224, 224]).asType('float32').div(255);
        result = await model.predict(image);
        console.log("Done");
        const outReshape = (tf.transpose(result, [2, 3, 1, 0])).reshape([224, 224, 1]);
        const outResize = tf.mul(tf.div(outReshape, tf.max(outReshape)), 255).asType('int32');
        await tf.browser.toPixels(outResize, canvas);
        setTimeout(predict, 0);
    }

</script>

</html>
